\chapter{Introduction} \label{ch:introduction}

The rise of the internet has introduced new challenges in consumer protection. One source of challenges is that cheap and efficient data collection has introduced unique avenues for revenue generation, resulting in an amazing variety of free or low cost services at a steep cost to consumer privacy. Another is that global marketplaces have led to large drops in prices as online platforms reduce market friction and increase competition; when this is combined with a powerful capacity to amplify speech, the internet has created a strong incentive to dishonestly push narratives and opinions for one's benefit, for example posting by fake reviews. Consumer protection laws are crucial to defend consumers in this challenging space.

A common approach for studying consumer protection issues on the internet is to collect data from websites and analyze the data to understand the issues. For small measurements, researchers may choose to perform their data collection by hand; however for datasets numbering in the hundreds of thousands of documents or larger, this approach soon becomes infeasible and researchers turn to web crawlers, automated collection tools. Either of these approaches assumes that the web is a static place; however, as we will show in this work, this is far from true. \textbf{When collecting web data to study consumer protection issues, researchers should consider repeated and longitudinal data collection rather than relying on a single, monolithic crawl.}

\section{Challenges in consumer protection}

\textbf{Consumer protection is a challenge that dates back millennia.}
Consumer protection issues likely date back to the beginning of commerce, even when face-to-face communication dominated. The story of the discovery of the Archimedes Principle could be interpreted as a consumer protection issue (if a monarch can be considered a consumer): the premise of the story is that the monarch tasked Archimedes with determining whether or not a gold crown was made of pure gold without damaging the crown~\cite{thompson2008archimedes}. This suggests that concern of deception in the marketplace is millennia old.  The standardization of weights and measures in ancient Greece and Rome can be interpreted as consumer protection rules, designed to ensure consumers received a fair share~\cite{smither2017roman}. Another example is the \textit{Manusmiriti} dating to around 700BCE in India, which lays out prohibitions on deceptive sales, for example of counterfeit goods or unclean food~\cite{devi2016legal}.

\textbf{Emerging technologies present new consumer protection challenges.}
In the years since, new technologies, especially communication technologies, have introduced consumer protection issues. New technologies open up opportunities for those seeking revenue at the expense of consumer welfare. There are many possible reasons for this, we suggest a few possibilities: consumers are not familiar with potential pitfalls; new technologies allow for greater scale, amplifying the marginal benefit of the smallest of gains; and that such scale may allow for diminished accountability, as alienating individual consumers has less impact. Ultimately, consumer protection issues come down to a conflict between the financial incentives of the merchant and the welfare of the consumer.

Scams are facilitated through technologies such as the postal system~\cite{uspismailfraud} and the telephone~\cite{ftcphonescams}. Improvements in banking led to the need to protect consumers against predatory or otherwise anti-consumer lending practices~\cite{eaglesham2011warning,doddfrank}. Research has shown that television programs have an influence on children's diets~\cite{morton1985television}. Most recently, the introduction of the internet has led to a host of new consumer protection issues. Websites offer poor privacy protections while collecting large swaths of data~\cite{estrada2017online}. New types of frauds and scams have emerged~\cite{ftcscamalerts}. Access to large audiences by individuals has led to challenges with sponsorship disclosures~\cite{ftc2021disclosures}. New methods of interacting with businesses have led to the phenomenon of dark patterns~\cite{darkpatternsorg}.

\textbf{Addressing consumer protection through legislation.}
Today, many jurisdictions ranging from local to national have passed consumer protection laws. In the United States, a leading national agency governing consumer protection on the internet is the Federal Trade Commission (FTC), established by the Federal Trade Commission Act of 1914~\cite{ftcact}. Many states have their own laws and entities focused on consumer protection~\cite{waller2011consumer}. The FTC has highlighted a number of issues at the intersection of the internet and consumer protection including fair reviews and adequate disclosure of privacy practices~\cite{ftc20approves,ftc21notice,ftc2021disclosures,ftc-privacy-survey1998,ftc-privacy-survey2000,ftc2021canspam,ftc1997principles}.

Consumer protection laws extend beyond the US and other Western countries. For example, Taiwan introduced a consumer protection law in 1994, although powerful industry lobbies and lack of consumer action made early enforcement challenging.~\cite{juang1997taiwan}. Nigeria implemented a federal commission to address consumer protection issues, the Federal Competition and Consumer Protection Commission; however, some academics have expressed skepticism at the decision to merge consumer protection and competition issues~\cite{tavuyanago2020interface}. This decision is not unprecedented---the US FTC also regulates both consumer protection and competition

Consumer protection issues are occasionally addressed through industry self-regulation, where members in a sector of industry collaborate to establish and enforce ethical and safety standards without government intervention. For example, the Better Business Bureau (BBB) is a non-profit that focuses on consumer protection through industry self-regulation. However, many have cast doubt on the effectiveness and fairness of the BBB, suggesting that consumer complaints through the organization are rarely resolved~\cite{fisher1999dissatisfied} and that the BBB's leadership is largely composed of former employees of those industries which generate large numbers of complaints~\cite{garrett2007debate}. AdChoices, a self-regulatory program co-founded by the BBB, has similarly received criticism over its effectiveness, in that its icon is small, nondescript, and frequently missing and that some members are not in compliance with opt-outs~\cite{hernandez2011tracking,garlach2018m,komanduri2011adchoices}.

These approaches to protecting consumers are important. However, simply passing legislation or engaging in self-regulation are not sufficient---it is crucial to understand how effective these measures are. By studying the regulated ecosystem, researchers can provide feedback to regulatory bodies to maximize the effectiveness of existing protections and draft new protections. This presents a potent opportunity for researchers to study the impact of consumer protection solutions and provide recommendations to industry and regulators.



\section{Academic study of consumer protection issues on the web}
There exists a rich academic literature studying consumer protection issues on the web. A central tool in studying the web is the web crawl: the usage of automated software, called crawlers, to fetch web pages of interest. When a web crawler includes a data extraction mechanism, called parsers, it is often called a ``web scraper.'' In the literature, web crawls are often conducted a single time, collecting a view of each targeted page. Possible reasons for this may include that the researchers assume a single timepoint is sufficient for their study, because of the labor and resource intensity in collecting additional data, because of the time investment required for longitudinal study, and because of the challenges involved in maintaining consistent data scope and quality throughout the study.

This view presents consumer protection issues as static. In contrast our work focuses on observing changes between web crawls. One type of change we show is that these issues form a moving target---for example, with evolving privacy laws---and that researchers need to consider not just the current state of affairs, but how that state is actively changing.

The second type of change we show is that we demonstrate that while web crawls are a powerful tool in analyzing consumer protection issues, the web is not a static place. In order to better grasp the issues at hand, it is important to take repeated measurements of the same pages. Just like you cannot measure the speed of an object with a single picture, some web phenomena cannot be observed in a single snapshot.

We explore the academic literature on consumer protection on the web in greater depth in Chapter \ref{ch:background}.

\subsection{The case for longitudinal study of consumer protection issues}
In this work, we argue that some consumer protection issues must be studied from a longitudinal perspective. Consumer protection issues often follow a cat-and-mouse game where consumers, merchants, fraudsters, and regulators take steps to respond to the actions of other actors and advance their goals. To understand these issues, we need to understand the ongoing changes and responses that led to the current state, and, hopefully, extrapolate to understand the impact of future actions.

We demonstrate how large-scale, longitudinal web data collection through web crawls allow for novel insights into consumer protection issues. As past precedent for longitudinal study, and especially longitudinal collection of data, for consumer protection issues on the web, consider the work of \citet{milne2006longitudinal}, who compared privacy policies in 2001 and 2003, finding that privacy policies were becoming more difficult to read. In the context of web tracking, \citet{krishnamurthy2009privacy} and \cite{sorensen2019before} use longitudinal web crawls to explore the changes in web tracking services. Other examples include the works of \citet{nathezhtha2019wc} and \citet{trabelsi2019monitoring} who encourage the use of continuous, and therefore longitudinal, web crawls to detect new phishing attacks and data breaches, respectively. 

\subsection{Ethical challenges in web crawling}
Web crawling studies are not without ethical challenges, and weighing the public benefit of research studies against the costs to both the targeted servers and users is a critical step. Web crawls can require a substantial amount of bandwidth. When too aggressive, a web crawler can act as a denial of service attack against the targeted websites. This can impede human users' access to the website as well as create cost and effort for the website's operators. To address this issue, it is important to throttle web crawls, especially when the target site shows signs of slowing down.

The data collected may contain personally identifiable information; while this data is usually publicly accessible, it is often collected and analyzed without consent. The scale of web crawls makes it often impractical to inform all users that their data is to be used for research, much less go through a full informed consent process. Prior work has shown that users are not always comfortable with their online data being used for research purposes, although the comfort depends on the context~\cite{fiesler2018participant}. Researchers should take extra caution to minimize singling out specific users and to consider carefully the impacts of data release.


\section{Contributions}

We divide our contributions into three components. First we present our methodology for collecting over 1 million privacy policies over more than 20 years, and we present our findings from our analysis of those privacy policies (Section \ref{sec:intro:privacypolicies}). Then we present our methodology for collecting over 12 million Yelp reviews and classification labels with spans up to eight years, and we present our findings from our analysis of those reviews (Section \ref{sec:intro:reviews}). Finally, we contribute our data and software for others to use in further research and to improve replicability of our work (Section \ref{sec:intro:datarelease}).


\subsection{Privacy policies over time: how have privacy policies changed over the years?} \label{sec:intro:privacypolicies}
We developed a web crawler which collects privacy policies from snapshots of websites hosted on the Internet Archive's Wayback Machine. Leveraging the Wayback Machine, we were able to collect privacy policies dating back to 1997. We used a combination of heuristics and machine learning to locate and identify privacy policies. 

Using our dataset of privacy policies, we demonstrate that privacy policies have become longer and less readable over time, doubling in length and rising a 1/4 of a grade level on the Flesch-Kincaid Grade Level readability score. We show that 20\% of privacy policies link to other additional privacy policies, further increasing reader burden. We support prior work in showing the widespread impact of GDPR; for example, we show that GDPR coincided with a large number of text updates, changes in language used, and shift in word count. We show that privacy policies underreport tracking technologies and third parties.  We show that advertiser-driven self-regulatory bodies have grown while first-party-driven self-regulatory bodies have stagnated. 

\subsection{Reviews in motion: a study of review reclassification} \label{sec:intro:reviews}
We developed a web crawler which collects reviews from Yelp. We collected reviews that Yelp recommends and does not recommend, which \citet{yelp2010recommendation} more thoroughly explains. We used two techniques to obtain a longitudinal aspect to our collection: the first is by comparing our collected data to that of prior work~\cite{mukherjee2013yelp}, the second is by repeated crawling of the same set of businesses for several months. We carefully chose our target sets to obtain three major cross-sections of the US: one target set focuses on the Chicago metropolitan area, giving us a view of reviews in a concentrated geographic area; the second is spread across the United States and stratified by population density; and the third is like the second but stratified by household income.

We leverage our reviews dataset to present the first study on review reclassification, the phenomena where a review changes classification from Recommended to Not Recommended or vice-versa. We show that reclassification is common over the long-term, and that multiple reclassification events can happen on the same review, even in the short term. This calls into question the validity of the studies that depend on Yelp's classification labels as ground truth~\cite{rayana2015collective,kc2016temporal,mukherjee2013yelp,zhu2021ifspard,shehnepoor2017netspam,yao2017automated}. Furthermore, it highlights Yelp's own uncertainty in their classifications.

We explore this reclassification phenomena deeper. We show that newer reviews experience more reclassification. We show that reviews and review classification are unevenly distributed across geographical regions. We explore the impact of mask-mentioning reviews and business masking rules on reviews, finding that mask mentions correspond to lower review ratings and that while masking rules also correspond to lower review ratings, this effect diminishes after Not Recommended reviews are removed.

\subsection{Data and software release} \label{sec:intro:datarelease}
For our work on both privacy policies and reviews, we contribute our collected data, our collection code, and our analysis code for others to use. Our datasets are, to our knowledge, the largest longitudinal datasets of their type.

We release the largest longitudinal dataset of privacy policies, available for use by the general public. Our dataset contains 1,071,488 documents from 131k websites. Our data can be requested at \par
\url{https://privacypolicies.cs.princeton.edu/}\\
 and our code can be accessed at \par
\url{https://github.com/citp/PrivacyPoliciesOverTime/}.\\ As of writing, we have received \policyrequests~ access requests.

We also release the largest longitudinal dataset of online reviews, available for use by researchers. Our dataset consists of three sub-datasets. The first is around 200k reviews that can be combined with the data from prior work~\cite{mukherjee2013yelp} to perform comparisons across an eight year timespan. The second sub-dataset consists of over 10 million reviews collected in 8 intervals over 11 months concentrated in the Chicago metropolitan area. The combined dataset consists of over 2.5 million reviews from businesses across the US, randomly sampled and stratified by both population density and household income, collected in 4 intervals over 4 months. Our data is pseudonymized by removing or replacing with numerical identifiers the fields we deem sensitive. Non-pseudonymous data is available with sufficient justification, such as research questions that cannot be answered with pseudonymous data and a plan to protect the data. Our data can be requested at\par
\url{https://sites.google.com/princeton.edu/longitudinal-review-data/}\\
and our code can be accessed at\par
\url{https://github.com/citp/LongitudinalReviews}.

We hope that our approach inspires future studies of consumer protection from a longitudinal angle. Our software and data releases lay the groundwork for such future study. 

\section{Structure}

In this work, our contributions are two large-scale, longitudinal web crawls and corresponding analyses studying consumer protection issues. 

We will explore background material on consumer protection in Chapter \ref{ch:background}, including an overview of relevant consumer protection laws (Section \ref{sec:background:laws}), the use of web crawls to study consumer protection (Section \ref{sec:background:crawls}), related work on privacy policies (Section \ref{sec:ppot:related}), and related work on online reviews (Section \ref{sec:rim:related_work}).

In Chapter \ref{ch:ppot} we present our work on longitudinal collection and analysis of privacy policies. We detail our methodology for collecting and filtering over 1 million privacy policies over a 20 year span in Section \ref{sec:ppot:dataset}. We then discuss our analyses of the data, including the contributions listed above, in Sections \ref{sec:ppot:doclevstatstime}, \ref{sec:ppot:analysis}. This chapter is adapted from \citet{amos2021privacy}.

We discuss our work on longitudinal collection and analysis of reviews on Yelp in Chapter \ref{ch:rim}. In Section \ref{sec:rim:dataset}, we present our methodology for collecting over 12 million online reviews on Yelp with three distinct cross-sections. Following that, we explore the data, including the contributions listed above, in Section \ref{sec:rim:results}. This chapter is adapted from \citet{amos2022reviews}.

Finally, we discuss takeaways and future work in Chapter \ref{ch:conclusion}. 