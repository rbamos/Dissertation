\section{Discussion} \label{sec:rim:discussion}
Online reviews are part of an actively evolving landscape with significant economic consequences. In this paper, we have examined this landscape from four different cross-sections: a course-grained eight year view;
a more fine-grained, eleven month view focused on one region; a four month view sampled from the whole US stratified by density; and a second four month view stratified by income. Each of these datasets is available for other researchers to use. 


Reviews on Yelp routinely move between classifications, in both directions, occasionally multiple times. Newer reviews are less stable in their classification, but even old reviews are still subject to occasional reclassification, even multiple reclassifications. These reclassifications are often connected to the review author. Density and income are connected to the number of reviews per business, the review classifications, and how frequently reviews are reclassified. We find both discussion of masks and, to a lesser extent, declaring a mask policy, impact the ratings given by reviewers. Our methodology can offer insight into an opaque process for reviewers, consumers, and businesses who might not understand why their review was blocked or why the reviews they see change.

Our results have implications for platforms and policymakers. Our reclassification results demonstrate the uncertainty in the recommendation process. We suggest other platforms consider a greater transparency model with their reviews, similar to Yelpâ€™s model---that platforms should remove, but still make accessible, reviews believed to be problematic. Furthermore, platforms should be cautious about changing classification until they are confident the review is not problematic. Platforms and regulators should consider carefully any discrepancies by density and income: are there steps that can be taken to address these inequities? Our observations surrounding masking policies suggest public backlash against a business's mask policy decision is negligible, which may be due in part to Yelp's classifier.




\subsection{Limitations} \label{subsec:rim:limitations}

Our study is limited to a single platform and a single country, so it may not be representative of trends on other platforms or other countries. Our study period includes the COVID-19 pandemic, a period of substantial social and economic disruption~\cite{altig2020economic,deb2020economic}. Furthermore, local median income and population density may not completely describe the businesses and reviewers; for example reviewers may travel from another area to the business or the area may be heterogeneous.

We expect that we may have missed reclassifications that occurred between crawl points and our crawler may have missed reviews (e.g. if the reviews reordered mid-crawl due to a new review). This means that some reviews may have been reclassified more frequently than we observed.

We also do not have a complete view of the review space---we have limited information on reviews removed for terms of service violations and no information on reviews removed by their author before our first crawl. Some of these reviews may be reviews of interest---for example, some problematic reviews may be removed entirely instead of made Not Recommended.

We hope that our work brings momentum to the longitudinal study of review platforms. To that end, our data, crawler, and analysis code is available at \url{https://sites.google.com/princeton.edu/longitudinal-review-data/}.