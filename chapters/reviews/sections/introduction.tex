\section{Introduction} \label{sec:rim:introduction}

Online reviews are an important source of consumer information, play an important role in consumer protection, and have a substantial impact on businesses' economic outcomes~\cite{luca2016reviews,anderson2012impact,un2003conpro}. This creates incentives for various parties to engage in problematic reviewing practices~\cite{streitfeld2012buy,miller19plastic}. These problematic reviews can encompass a large spectrum of behaviors, such as the creation of new accounts for posting fake reviews, hijacking legitimate accounts, compensating real users for posting favorable reviews, incentivized reviews, reviews written by friends or competitors, hiding of negative reviews, requesting negative reviews be handled confidentially, or reviews that are not relevant to the product or service. This is subject of ongoing interest to regulators~\cite{jindal2008opinion,yelpwhyrec,ftc21notice,ftc21fashion,yeung2021Bad}. To address the challenges posed by problematic reviews, review platforms have created classification systems to present users with the best reviews to make informed decisions. A wealth of prior work has taken a variety of approaches to understanding online reviews and the underlying classification systems. This work ranges from studying the factors motivating both honest and problematic reviews to exploring how to detect and generate opinion spam or fake reviews~\cite{jindal2008opinion,yoo2008motivates,baginski2014exploring}. Most of this work examines reviews and review classification systems from a single time-point; however, this perspective is incomplete---these classification decisions are not static.

\textbf{Contributions.} In this work, we move from a view of reviews at rest to a view of reviews in motion: how do reviews move between classes? We explore the dynamic classification of reviews through the collection and analysis of three novel, longitudinal datasets, focused on Yelp. We sought to observe the changing nature of review classification, and thus we collected datasets in which we observed the reviews for a fixed set of businesses across multiple time-points. Our datasets total over 12.5 million total reviews and two million unique reviews, with observations over timescales ranging from four months to eight years. The longitudinal aspect of data allows us to observe the movement of reviews between classifications, which we call ``reclassification.'' We take advantage of our dataset to study other aspects of online reviews: our carefully chosen cross sections allow us to approach questions of demographic interactions with reviews, and our timing allows us to explore questions around COVID-19's impact on reviews.

Our contributions are as follows:
\begin{enumerate}
    \item The largest longitudinal dataset of reviews, tracing over 2M reviews across over 11k businesses.
    \item The first study of review reclassification and exploration of this yet-unstudied phenomenon. We find reclassification rates between 0.54\% over four months to 8.69\% over 8 years. Furthermore,  newer reviews are more affected, and that classification appears to happen mostly at an author level.
    \item An exploration into the impacts of income and density, showing disparities in review frequency and reclassification by both income and density.
    \item An investigation of the impacts of mask discussion and rules, showing that the classifier erases rating differences for businesses requiring masks.
\end{enumerate}


\textbf{Implications.} Our results demonstrate that reviews are routinely reclassified, occasionally multiple times. Understanding these reclassifications helps to highlight consumer issues around review classifiers and the challenging nature of building robust classifiers. The magnitude of reclassification also calls into question the validity of the studies that depend on Yelp's classification labels as ground truth~\cite{rayana2015collective,kc2016temporal,mukherjee2013yelp,zhu2021ifspard,shehnepoor2017netspam,yao2017automated}. Multiple-reclassifications (reviews with 2+ reclassifications) suggest that reclassifications do not always move towards ground truth, especially considering the 1,233 multiple-reclassifications we identified. Balancing the issues of fairness to legitimate reviewers with the need to remove problematic reviews is a challenging problem, and our work sheds light on the challenges faced.

