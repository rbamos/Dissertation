\section{Results} \label{sec:rim:results}

In this section, we explore three key questions:

\textbf{How extensive are review reclassifications on Yelp?} Yelp presents most reviews as either ``Recommended'' or ``Not Recommended,'' and Yelp moves reviews between those categories. However, Yelp does not discuss how frequently this movement occurs. Reclassifications are an indicator of the confidence Yelp has in its classifications, the challenging nature of the problem, and the effort Yelp puts into updating its classifier. Furthermore, reclassifications may frustrate consumers and businesses.

\textbf{How do density and income impact reviews on Yelp?} Disparities in reviews in different regions are an important part of understanding equity on review platforms. For example, it could be possible that certain regions are disproportionately targeted by malicious reviews, or that Yelp's classifier is tuned towards a certain subset of regions.

\textbf{How do mask discussions and requirements impact businesses on Yelp?} With the ongoing COVID-19 pandemic, masks have been a controversial topic~\cite{pascual2021toxicity}; how have mask requirements and mask discussion affected businesses?

For our analysis, we use SciPy~\cite{2020SciPy-NMeth} for statistics, Pandas~\cite{mckinney-proc-scipy-2010} for data processing, and Seaborn~\cite{waskom2020seaborn} for visualizations. We excluded businesses which have no reviews. Unless otherwise stated, we use both Recommended and Not Recommended reviews. All p-values have been corrected for 6 hypotheses using the Holm-Bonferroni multiple hypothesis correction method with a significance level of $\text{p}<0.05$~\cite{seabold2010statsmodels}.


\subsection{Review reclassification} \label{subsec:rim:review_reclassification}

Although Yelp has said that it reclassifies reviews~\cite{yelp2010recommend}, it has not specified the frequency or nature of these changes. Reclassification details could hint at Yelp's approach to classification and details of its classifier, such as the features it considers. The nature of the classification changes could indicate whether Yelp errs towards over- or under-filtering. The frequency of reclassification, illustrated in Table \ref{tab:reclassification}, calls into question the validity of the studies that depend on Yelp's classification labels as ground truth~\cite{rayana2015collective,kc2016temporal,mukherjee2013yelp,zhu2021ifspard,shehnepoor2017netspam,yao2017automated}. As such, it is important to dive into this phenomenon and understand the factors connected to reclassification.

\begin{table}[t]
    \centering
    \caption[Reclassification of reviews between 2012 and 2020]{Reclassification of reviews between 2012 and 2020. Only includes reviews present in both snapshots.}
    \label{tab:reclassification}
    \begin{tabular}{lcc}
		  \toprule
      & Recommended &  Not Rec. \\
			& (2012) & (2012) \\
			\midrule
      Recommended (2020) & 56,048 & 3,566 \\
      Not Recommended (2020)& 2,249 & 5,059\\
			\bottomrule
    \end{tabular}
\end{table}

\textbf{In the long run, reviews are disproportionately reclassified as Recommended from Not Recommended.}
We first approach reclassification in the long timescale with the EYG dataset. Table~\ref{tab:reclassification} shows how reviews have been reclassified between the two snapshots: most reviews receive the same classification in both snapshots, but a significant number of reviews are classified differently between them ($\chi^2$ test with 1 degree of freedom: $\text{p}<<1\text{e}-5$). We note that more reviews are reclassified as Recommended from Not Recommended than vice versa, which is especially interesting considering that 87.6\% of the reviews were Recommended in the 2012 snapshot. Proportionately, we observe that 3.9\% of the reviews that were Recommended in the 2012 snapshot were Not Recommended in the 2020 snapshot, while 41.3\% of the reviews that were Not Recommended in the 2012 snapshot were Recommended in the 2020 snapshot.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{chapters/reviews/figures/filtered_proportion_density.pdf}
    \caption[Probability density of Not Recommended review percentage for a business]{Probability density of Not Recommended review percentage for a business, 2012 and 2020 data. Lines are the kernel density estimates.}
    \label{fig:filtered_density}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{chapters/reviews/figures/filtered_vs_count.pdf}
    \caption[Number of reviews versus the percentage of reviews that were Not Recommended for a business]{Number of reviews versus the percentage of reviews that were Not Recommended for a business, 2012 and 2020 data.}
    \label{fig:count_vs_perc}
\end{figure}

\textbf{The percentages of Not Recommended reviews per business tend to converge with more reviews.} To understand whether review classification has changed at a business level, we examined the distribution of percentage of Not Recommended reviews per businesses and the connection between number of reviews and percentage Not Recommended per business. Figure~\ref{fig:filtered_density} shows the distribution of percentage Not Recommended by business. The median percentage Not Recommended is similar (0.122 and 0.115), but the distributions are distinct (Kolmogorovâ€“Smirnov test $\text{p}<0.05$). In 2012, a number of business have no Not Recommended reviews. Some of these results may be explained by the business having fewer reviews in 2012 than 2020. Figure~\ref{fig:count_vs_perc} shows the connection between the number of reviews on a business and the percentage Not Recommended. Most businesses converge to around the same percentage Not Recommended with enough reviews. This convergence appears tighter among the 2020 data. We note that there is no significant correlation between the number of reviews and the percentage Recommended for the 2012 data (Spearman's correlation $\rho = 0.13$, $\text{p} = 0.06$), but there is a significant, negative correlation for the 2020 data ($\rho = -0.31$, $\text{p}<1\text{e}-4$)---the more reviews a business has, the smaller the proportion of Not Recommended reviews. The correlation for the 2020 data may be more significant because the businesses are more established.

\begin{table}[t]
    \centering
    \caption{Frequency of reclassification patterns observed in Chicago data over 5 timepoints. ``R'' denotes Recommended, ``N'' denotes Not Recommended.}
    \label{tab:reclassification_patterns}
    \begin{tabular}{llr}
    \# changes & Pattern & Count \\
	 \hline
	 0 & R & 1,235,194\\
	 & N & 148,278\\
	 & \textit{Total} & 1,383,472 \\
	 \hline
	 1& R $\to$ N & 4,953\\
	 & N $\to$ R & 5,573\\
	 & \textit{Total} & 10,526\\
	 \hline
	 2& R $\to$ N $\to$ R & 706\\
	 & N $\to$ R $\to$ N & 373\\
	 & \textit{Total} & 1079\\
	 \hline
	 3+ & R $\to$ N $\to$ R $\to$ N & 60\\
	 & N $\to$ R $\to$ N $\to$ R & 75\\
	 & R $\to$ N $\to$ R $\to$ N $\to$ R & 14\\
	 & N $\to$ R $\to$ N $\to$ R $\to$ N & 15\\
	 & N $\to$ R $\to$ N $\to$ R $\to$ N $\to$ R & 2\\
	 & \textit{Total} & 157\\
	 \hline
    \end{tabular}
\end{table}

\textbf{Many reviews are reclassified, a few are reclassified frequently.} To investigate the frequency and scale of reclassification on shorter timescales we investigate reviews from the CHI dataset. Table \ref{tab:reclassification_patterns} shows how frequently reviews were reclassified. In our study period, around 0.8\% of reviews were reclassified. A small fraction of reviews undergo a substantial number of changes. This is especially interesting in light of the shorter study period and the limited number of time-points---a few reviews changed classes almost every measurement. These reviews may be cases that are particularly hard for Yelp to classify. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{chapters/reviews/figures/filtering_changes.pdf}
        \caption[Change in recommendation status by the year reviews posted]{Change in recommendation status by the year reviews posted. The blue line represents reviews Not Recommended in 2012; the orange and green lines represent reviews that were Recommended and Not Recommended, respectively, in 2012 but reclassified in 2020.
    }
    \label{fig:filtered_change}
\end{figure}

\textbf{In the long run, newer Not Recommended reviews are more likely to be reclassified.} To explore the relationship between reclassification and review age, we grouped reviews in EYG by the year posted. Within each group, we measured the percentage of reviews Yelp Recommended in 2012 and the percentage reclassified in 2020 from Recommended and Not Recommended (Figure \ref{fig:filtered_change}). Note that 2004 has just five reviews, and all were Recommended in 2012. The trend of the percentage Not Recommended in both 2012 and 2020 (green) suggests that Yelp was more likely to reclassify newer reviews from Not Recommended to Recommended. Yelp could have had more time to examine older reviews by 2012, or older, less sophisticated fake reviews may have resulted in fewer filtering errors to correct. Alternatively, this could stem from the use of review age or correlated factors (e.g., the number of reviews per author) in classification: Yelp considers whether a reviewer is ``established'' in recommending reviews, which is something Yelp claims to do~\cite{yelpwhyrec,yelprecommendationsoftware}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{chapters/reviews/figures/reclassification_by_date_grouped_chicago.pdf}
    \caption[Cumulative percentage of reviews with a given number of reclassifications that were posted by a given date]{Cumulative percentage of reviews with a given number of reclassifications that were posted by a given date. For example, by the start of 2018 approximately 20\% of reviews with 2 observed reclassifications had been posted.}
    \label{fig:reclassification_by_date_chicago}
\end{figure}

\textbf{Newer reviews are more likely to undergo repeated reclassification, but the chance of reclassification persists over time.} Figure \ref{fig:reclassification_by_date_chicago}, shows reviews which undergo more frequent reclassification tend to be newer. This supports the idea that Yelp increases its confidence in classifying reviews as the reviews age. Perhaps because Yelp has observed more activity from the author's account or the business. We note that we still see reclassifications of reviews dating back to 2005, suggesting that a review's classification is never fully stable. We note a small artifact in the upper-right corner: the 1, 2, and 3+ lines rise above the 0 line. We expect this artifact because the newest reviews cannot have 1, 2, or 3+ reclassifications, since we have not made as many observations of them.

Given these results showing that newer reviews are more likely to undergo reclassification and that the percentage reclassified for the EYG (8.69\% over 8 years or 0.09\% per month) and CHI datasets (0.87\% over 11 months or 0.08\% per month) is similar, it seems likely that Yelp's classifier is either more stable today or that Yelp performed a major overhaul between the EYG collection time-points. It is also possible that the EYG sample was disproportionately subject to reclassification.

\textbf{Review classes follow the author.} In order to determine if reclassifications are performed at the author level, we investigated whether authors who have a review classification change are likely to have their other reviews match the new classification. In the 1,175 cases in which an author with multiple reviews had a review reclassified, 924 had all of their reviews match after reclassification. The average percentage of an author's review pairs that matched classification was 95.1\% for Recommended reviews and 94.7\% for Not Recommended reviews. This suggests that classifications follow the author. We also observe some multiple-reclassifications at the author level, indicating misclassifications, which are harmful to legitimate authors whose reviews are hidden. For example, we found an author with 18 reviews temporarily reclassified as Not Recommended. Such double reclassifications indicate a misclassification at some point. If the author is a legitimate author, they may be discouraged by this reclassification and choose not to engage in further reviewing. Furthermore, their ability to inform other consumers was greatly diminished during the period where their reviews were classified as Not Recommended.

Reclassification may act as a potential point of frustration or even chilling for legitimate users and businesses, and can lead to questions of fairness---while the classifier may work well in the average case, the worst case is experienced by real people and thus matters. As two examples of significant swings in rating, we found a business which had its rating go from 3.5 to 4.5 after a reclassification and another from 2.5 to 1, suggesting that either Yelp's initial or updated rating failed to represent legitimate reviewer attitudes.  It also leads to questions about consumer protection in access to information about products---reclassification shows that consumers are not getting an entirely accurate picture. We did not find that any particular timepoint had significantly more reclassifications.

\textbf{Examples of reclassifications.} To understand the variety of factors preceding a reclassification, next we will examine a few reviews that were reclassified. We focus on reclassifications around account changes, which we defined as changes in review count, friend count, and photo count. Yelp may have access to other account changes we do not have access to, such as login data and review edits. We observe that reclassifications can occur with no account changes, shortly after account changes, and with a delay after account changes.

Some reclassifications occur despite no observed account changes. For example, we observed two reviews posted in July 2012 and April 2017 with ratings of 1 and 4, friend counts of 0 and 3, and review counts of 3 and 22, respectively. Both were Recommended in CHI 1-4 and Not Recommended in CHI 5-8, despite no account changes.

Other reclassifications occur after an account change. For example, we observed a review posted in October 2020 with a rating of 2 which was Not Recommended in CHI 1-4, but Recommended in CHI 5-8, after the user posted 3 more reviews. This user had approximately 400 friends and 1 review in CHI-1.

Sometimes the changes lag behind account changes. For example, we observed a review from July 2013 with a rating of 5 which was changed to Recommended in CHI 6 after the author posted 2 reviews between CHI 2 and CHI 3. In CHI-1, the user had approximately 180 friends and 9 reviews.


\subsection{Density and income impacts}

We investigated how density and income impact reviews using our UDIS study. Demographic disparities are indicative of issues of fairness in the online review space---all consumers should have equal access to both write and read reviews relevant to them, but, as we show, some consumers may have fewer relevant reviews in their area or may find their reviews more likely to be hidden. While our data is not sufficient to determine the root causes of disparities, we can identify them for further study.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{chapters/reviews/figures/reviews_per_business_stratified.pdf}
    \caption[Reviews per busienss]{Reviews per business. Each data point is the number of reviews for one business. }
    \label{fig:reviews_per_business_stratified}
\end{figure}

\textbf{Demographic factors correlate with the number of reviews on each business.} We investigated the number of reviews per business in Figure \ref{fig:reviews_per_business_stratified}. Both low income and low density areas have fewer reviews per business than higher income and higher density areas, and the gap is wider for income. This could be because higher income areas typically have businesses with more time on the platform---for the highest income stratum the median oldest review for each business (3,425 days) is 30\% older than for the lowest income stratum (2,638 days). This relationship is not as strong in the density experiment: the middle density stratum has the oldest reviews (3,245 days), slightly higher than the highest stratum (3,066 days) and much higher than the lowest stratum (2,704 days). The disparity in number of reviews means consumers in these areas may have less access to reviews, impacting their ability to make informed decisions.

 \begin{figure}[t]
     \centering
     \includegraphics[width=0.9\columnwidth]{chapters/reviews/figures/percentage_recommended_per_businesses_extended.pdf}
     \caption[The proportion of reviews Recommended per business]{The proportion of reviews Recommended per business. Each data point represents one business.}
     \label{fig:percentage_recommended_per_businesses_extended}
 \end{figure}
 
\textbf{Demographic factors correlate with the percentage of reviews Recommended for each business.} We examined how the percentage of reviews that are Recommended per business varies by income and density in Figure \ref{fig:percentage_recommended_per_businesses_extended}. The range in median percentage Recommended is tighter for density---it ranges from 80\% (Bottom 20\%) to 86\% (Top 20\%)---whereas the range is larger for income, ranging from 78\% (40-60\%) to 87\% (Top 20\%). We observe that higher density and higher income areas generally have a higher percentage of recommended reviews. Possible causes of this range from a lower concentration of problematic reviews to better tailoring of the recommendation algorithms for those areas.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{chapters/reviews/figures/stratified_reclass_swaps_usa.pdf}
    \caption[Average number of reclassifications per unique review per stratum]{Average number of reclassifications per unique review per stratum. Black bars indicate the 95\% confidence interval}
    \label{fig:stratified_reclass_swaps_usa}
\end{figure}

\textbf{Demographic factors correlate with the frequency of reclassifications.} In Section \ref{subsec:rim:review_reclassification}, we explored the frequency and factors surrounding reclassification. To test whether Yelp reclassifies reviews for businesses in regions with certain income or density attributes more frequently, we looked at the average number of reclassifications per review in each stratum for both the UIS and UDS crawls (Figure \ref{fig:stratified_reclass_swaps_usa}). Less dense and lower income regions experience more reclassification. The 60-80\% strata are an outlier in both cases---the 60-80\% density stratum experiences significantly more reclassifications while the 60-80\% income stratum experiences significantly less than its neighbors, but on par with the top income stratum, which warrants further research. These disparities invite questions as to why they arise: is there something inherent about these markets that leads to more challenging-to-classify reviews, or is Yelp's classifier not well tuned to them?



\subsection{Masking}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{chapters/reviews/figures/proportion_Masks_by_Rating_usa.pdf}
    \caption[Review rating and masking]{The proportion by rating of reviews on or after August 6, 2021 on businesses that (1) require masks (2) do not require masks (3) do not list a requirement for masks. In addition, the proportion of reviews on or after March 1, 2020 that (4) mention masks (5) do not mention masks. Black bars indicate the 95\% confidence interval.}
    \label{fig:proportion_Masks mentions_masks required_by_rating_usa}
\end{figure}

In response to the COVID-19 pandemic, Yelp added an option for businesses to specify a mask policy in August 2021. Because this coincided with our longitudinal data collection, we studied reviews that mention masks and business with listed mask policies. We used our UDIS-4 crawl data because that crawl started after Yelp added the option. 

We determined whether a review mentioned masks by tokenizing the review and lemmatizing the tokens. If any lemma matched ``mask,'' we considered that review to mention masks. We find that 90.2\% (2,845) of UDIS reviews mentioning masks occur on or after March 1, 2020. We manually examined a random sample of 20 such reviews from before March 1, 2020: 16 used ``mask'' to describe covering a taste or odor, 1 in a COVID-19 context, and 3 to describe costumes. We examined a random sample of 20 such reviews from on or after March 1, 2020; all 20 of them used ``mask'' in a COVID-19 context. To determine whether a business has a masking policy, we used the business amenities as described in Section \ref{subsec:rim:crawling}. We found 837 businesses requiring masks, 174 not requiring masks, and 4,666 with no listed policy.

\textbf{Mask policies have little correlation with rating, as long as one is present. Discussions of masks correspond to lower ratings.} We show how both customer mask requirements and mask discussion affect rating in Figure \ref{fig:proportion_Masks mentions_masks required_by_rating_usa}. Reviews mentioning masks have a lower rating, and this relationship remains after removing Not Recommended reviews. Having a mask requirement results in a non-significant rating change (means 3.89 and 4.00, Spearman correlation $\rho = -0.04$, $\text{p}=0.06$), and this relationship vanishes after removing Not Recommended reviews (mean 3.92 and 3.90, Spearman correlation $\rho = -0.01$, $\text{p}=0.79$). This suggests Yelp's filter may have a mild effect of protecting restaurants requiring masks. Listing any policy correlates with higher ratings; this could be explained by the overall correlation between higher ratings and more listed amenities: the Spearman's rank correlation between the rating and the number of amenities is $\rho=0.119$ $(\text{p}<<1\text{e}-5)$. While more data is needed to establish statistical significance, our results seem to conflict with the results of \citet{kostromitina2021his}, who found that reviewers generally reviewed more favorably those businesses with better COVID safety protocols; this could be because of their broader keyword list, which encompassed concepts such as takeout. Consumer perceptions of businesses' health and safety protocols are important for policy makers who might wish to rely on the free market rather than statute to prescribe health and safety practices.



