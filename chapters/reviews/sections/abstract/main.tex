Online review platforms constantly make decisions; for example, which reviews should they present to readers? Research on reviews and review platforms often derives insights from a single web crawl, but the decisions those crawls observe may not be static. A platform may feature a review one day and filter it from view the next day. An appreciation for these dynamics is necessary to understand how a platform chooses which reviews consumers encounter and which reviews may be unhelpful or suspicious. We introduce a novel longitudinal angle to the study of reviews. We focus on ``reclassification,'' wherein a platform changes its filtering decision for a review. To that end, we perform repeated web crawls of Yelp to create three longitudinal datasets. These datasets highlight the platform's dynamic treatment of reviews. We compile over 12.5M reviews---more than 2M unique---across over 10k businesses. Our datasets are available for researchers to use.

Our longitudinal approach gives us a unique perspective on Yelp's classifier and allows us to explore reclassification. We find that reviews routinely move between Yelp's two main classifier classes (``Recommended'' and ``Not Recommended''), and authors' reviews tend to move as a group between classes. Some reviews move multiple times: we observed up to five reclassifications in eleven months. Our data suggests demographic disparities in reclassifications, with more changes in lower density and low-middle income areas. Because our web crawls coincided with the COVID-19 pandemic, our data also allowed limited exploration of the impact of mask policies and discussions on reviews.