\section{Online reviews} \label{sec:rim:related_work}

As the study of online reviews is the focus of Chapter \ref{ch:ppot}, we go deeper in depth on the literature surrounding reviews. Extensive academic literature from multiple disciplines studies online reviews and fake reviews. We highlight four primary areas of prior work: longitudinal study of reviews, demographics and reviews, problematic reviews, and incentives for reviewing.

\textbf{Longitudinal study.} Some researchers have performed longitudinal analyses on reviews with a single snapshot, for example by using review dates~\cite{bakhshi2014demographics,ye2016temporal,wang2017temporal}.  Other studies have linked datasets together to gain a better vantage on the review landscape, but still rely on a single snapshot for each data point. For example, \citet{nilizadeh2019think} used reviews from multiple different review platforms along with change point analysis to find fraudulent reviews. To the best of our knowledge, no other studies focus on review reclassification. Yelp acknowledges that reviews are classified by an automated system and sometimes reclassified, but Yelp does not disclose the frequency with which this happens \cite{yelpwhyrec,yelpwhychange}.

\textbf{Demographics and reviews.} Ensuring equal access to and treatment by technology is an important equity issue. \citet{baginski2014exploring} explored the hypothesis that, within Franklin County, OH, low income areas have fewer reviews. Instead, they found that there were strong concentrations of reviews, and suggested that technological adoption may play a role. Van Velthoven et al.~\cite{van2018cross} support this hypothesis in their work exploring reviews and ratings in a medical setting; they also do not find a strong link between income or urban/suburban living and the frequency of review authorship. In contrast, \citet{bakhshi2014demographics} find that, among restaurant reviews, local population density has a small but statistically significant effect on review count, but not on rating.  However, \citet{sutherland2020topic} show, using topic modelling, that in hotel ratings rural and metropolitan settings and decor are important discussion points for consumers. Our work helps improve the perspective on how demographics shape reviews.

\textit{Problematic reviews.} Problematic reviews have served as a persistent challenge in the review landscape, largely in the context of detecting fake reviews. \citet{ott2012estimating} estimated that fake reviews occurred at a rate of 2-6\% across six platforms, but other estimates of fake reviews reach 50\%-70\%~\cite{dwoskin2018merchants,elliott2018trust}. Yelp reports filtering about one quarter of its reviews~\cite{yelp2010recommend}. A wealth of research focuses on the problem of detecting these fake reviews~\cite{jindal2008opinion,martens2019towards,ye2016temporal,shehnepoor2017netspam,kumar2018rev2,harris2012detecting,mukherjee2013yelp}. Some works have taken an adversarial approach, focusing on generating fake reviews rather than detecting them~\cite{adelani2020generating,juuti2018stay,yao2017automated}.

The filtering of problematic reviews introduces new challenges. \citet{eslami2019user} show that some users find the automated filtering system on Yelp to be frustrating and discouraging. However, many users also view the system to be an essential protection against problematic reviews. Many users disliked the opacity of filtering decisions.

Some prior work has attempted to audit existing problematic review classifiers. For example \citet{kamerer2014understanding} attempted to identify both review-based features and reviewer based features that predicted a Not Recommended classification on Yelp. Based on the features they examined, they find that reviewer-based features are more predictive of classification. Similarly, \citet{mukherjee2013yelp} found that, while there are linguistic differences between Recommended and Not Recommended reviews, the reviewer-based features were more predictive of Yelp's classification.

One challenge in analyzing fake reviews is the absence of ground-truth data. Fake reviews may be designed to fool even humans~\cite{ott2011finding}, and only the author of a review may know its authenticity with certainty. \citet{wang2016real} obtained ground-truth data by using leaked data from fake reviewers. \citet{martens2019towards} posed as customers to fake review providers to identify other fake reviews. \citet{ott2011finding} had study participants create fake reviews. Other studies rely on suspected---but unconfirmed---fake reviews. ~\citet{jindal2008opinion} used ``obviously'' fake reviews (e.g., duplicates) on Amazon to train a model to find other fake reviews. \citet{mukherjee2013yelp} and \citet{rayana2015collective} rely on Yelp's classifications for analysis.

Our work most closely resembles the work of \citet{kamerer2014understanding} and \citet{mukherjee2013yelp}, in that we study Yelp's deployed classifier, particularly focusing on the dynamic nature of decisions made.

\textbf{Incentives.} A number of economics studies have tried to understand the incentives behind posting both legitimate and problematic reviews. \citet{yoo2008motivates} showed that most consumers post reviews for themselves, to help the company, and to protect other consumers, while a smaller portion do so as retribution for poor service. \citet{luca2016fake} studied the economic incentives for problematic review posts, concluding that chain restaurants, restaurants with stable ratings, and restaurants with not much competition are less likely to have fake reviews posted on their pages. This literature is crucial for interpretation of the results of our study.

\textbf{COVID-19.} As a relatively recent phenomena, studies relating to the impacts of the COVID-19 pandemic are still emerging. Some prior work studying the impact of COVID-19 on online reviews exists; for example \citet{kostromitina2021his} using topic modeling to show that high rating reviews during the pandemic often discussed take-out services and follow safety guidelines.