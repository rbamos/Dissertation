\chapter{Background and related work} \label{ch:background}

In this section, we will cover the background material needed to place this work into context. First, we will cover some of the definitions of consumer protection in Section \ref{sec:background:definition}. Then we will cover some of the most relevant consumer protection laws and agencies in Section \ref{sec:background:laws}. Next, we will survey the academic literature on consumer protection issues on the internet, with a focus on exploration of such issues through the lens of web crawls in Section \ref{sec:background:crawls}. Finally, we will cover the important literature for contextualizing our contributions to understanding privacy policies in Section \ref{sec:ppot:related}, and online reviews in Section \ref{sec:rim:related_work}.

% Consumer protection and other web issues
% (strategy -- go through the proceedings of ConPro workshops \url{https://www.ieee-security.org/TC/SPW2021/ConPro/program.html})

\section{What is consumer protection?} \label{sec:background:definition}

One key definition of consumer protection dates to U.S. President John F. Kennedy's ``Consumer bill of rights'' speech. This speech established 4 core rights of consumers: \cite{kennedy1962special}
\begin{itemize}
\item ``The right to safety''
\item ``The right to be informed''
\item ``The right to choose''
\item ``The right to be heard''
\end{itemize}

The United Nations Guidelines for Consumer Protection expanded these guidelines, first in 1985, again in 1999, and revised them in 2015. Currently these guidelines are: \cite{un2003conpro}
\begin{itemize}
\item ``Access by consumers to essential goods and services''
\item ``The protection of vulnerable and disadvantaged consumers''
\item ``The protection of consumers from hazards to their health and safety''
\item ``The promotion and protection of economics interests of consumers''
\item ``Access by consumers to adequate information to enable them to make informed choices  according to individual wishes and needs''
\item ``Consumer education, including education on the environmental, social, and economic consequences of consumer choice''
\item ``Availability of effective consumer dispute resolution and redress''
\item ``Freedom to form consumer and other relevant group or organizations to present their views in decision-making processes affecting them''
\item ``The promotion of sustainable consumption patterns''
\item ``A level of protection for consumer using electronic commerce that is not less than that afforded in other forms of commerce''
\item ``The protection of consumer privacy and the global free flow of information''
\end{itemize}

In Chapters \ref{ch:ppot} and \ref{ch:rim}, we will be focused on two areas of interest on the internet: privacy policies and online reviews. From a consumer protection standpoint, privacy policies serve a number of roles. They inform consumers about company practices, they help protect consumer privacy through transparency, and, in some cases, provide options for managing disputes. Online reviews also help inform consumers, ensure free flow of information, and help consumers promote their economic interests. 


\section{Consumer protection laws} \label{sec:background:laws}
In this section we will highlight the most relevant consumer protection laws and agencies to our work.

In the United States, the federal agency that enforces much of the consumer protection issues we focus on in this paper is the Federal Trade Commission (FTC), established by the Federal Trade Commission Act~\cite{ftcact}.

One of the earliest actions in online privacy was the FTC's letter that threatened legal action against KidsCom, a website for 5-14 year old children~\cite{ftc1997principles}. The FTC stated that failure to disclose the marketing uses of collected information from the children constituted a violation of Section 5 of the FTC act, which regulates unfair and deceptive acts. This letter was soon followed by legislation from Congress: the Children's Online Privacy Protection Act ~\cite{coppa}. Unfortunately, compliance with COPPA is still an uphill battle; \citet{reyes2018won} found that a majority of child-direct apps contain potential COPPA violations. More recently in the US, the Californica Consumer Privacy Act (CCPA) California Online Privacy Protection Act (CalOPPA) has brought a number of privacy protection measures to residents of California~\cite{caloppa}. While CCPA and CalOPPA are unenforceable against those living outside California, they may raise the standards of privacy for those in other states, perhaps by inspiring other states to follow suit.

Perhaps the most significant privacy law worldwide is the European Union's General Data Protection Regulation. This legislation provided a massive overhaul to the privacy rights of European citizens~\cite{gdpr,hoofnagle2019european}. The GDPR includes many privacy protections, including giving consumers the right to object to the use of their data outside of certain purposes, the right to access their data, the right to erase their data.

We cover the laws around consumer privacy in more detail in Section \ref{sec:ppot:background}

% Relevant to our work:
% FTC act (include text of section 5?)
% - Privacy law (forward reference) and compliance
%     - Include COPPA, GDPR, CalOPPA
%     - \cite{reyes2018won} automated detection of COPPA compliance
    
% Other major bodies:
% - FDA, FDIC, CPSC

% Other countries:
% - Nigeria: Federal Competition and Consumer Protection Commission (FCCPC)/Federal Competition and Consumer Protection Act
% - Taiwan: Consumer Protection Law

%Worth mentioning BBB as a non-profit self-regulatory body?

\section{Web crawls to study consumer protection issues} \label{sec:background:crawls}
Consumer protection issues on the internet have been studied from many angles. In this section, we will primarily focus on how web crawls have been useful in studying issues of consumer protection, especially on the internet. \citet{ahmad2020apophanies} show that nearly 16\% of 2015-2018 publications at major internet measurement, security, privacy, and networking conferences depend on data obtained from a web crawl. We'll examine how internet measurements, especially web crawls, are important in shaping our understanding of consumer protection issues; specifically security, privacy, spam, dark patterns. In almost every web-crawler based study we examined, the data was collected in a single pass, with no comparison points of the same page being taken again at a later time.

\subsection{Security}
Internet measurements are an important tool for studying security problems at scale. When researchers identify a vulnerability, they may wish to understand how widespread the vulnerability is. This can help researchers understand the degree of exposure the public faces to the vulnerability.

\textbf{Cryptography.} One such area of focus is cryptographic vulnerabilities. \citet{heninger2012mining} showed that many websites' RSA keys are vulnerable to attack, for example sharing a common factor with another key. This can be used to recover the private keys for these websites, allowing anyone with that key to impersonate the website. The team scanned the entire IPv4 address space to collect 11 million public keys from TLS and SSH servers and found at least 0.5\% of TLS hosts and 1.6\% of SSH hosts vulnerable to attack. \citet{kharche2021study} used pre-crawled data from Internet Archive to explore the security of cryptography algorithms presented on Stack Overflow, a platform where programmers exchange knowledge such as code snippets. They found several instances of posts on Stack Overflow containing references to outdated cryptography algorithms with no community warnings that the posts were outdated. These errors may propagate into projects containing those snippets.
 
\textbf{Phishing.} % \raNote{This needs to be seriously reworked to be more about web crawls, or at least one study with web crawls}
Many studies concern internet safety. Phishing is a core issue in internet safety. In a phishing attack, the attacker uses a false identity to fool the user into providing the attacker information. Often this type of attack involves clicking links to websites which are visibly similar to a website the target uses, but are actually controlled by the attacker. \citet{hong2012state} provides an overview of phishing attacks, including the anatomy of attacks, why the attacks are successful, and the scale of the attacks. While the blame for phishing attacks' success may be pinned on under-educated users, attackers often take advantage of psychological tactics and poor user interface design~\cite{dhamija2006phishing}. In spite of decades of research into phishing, phishing is far from a solved problem. \citet{das2019sok} outlines the modern issues users face from phishing today. While a vast array of machine learning tools have been developed to detect phishing attempts before they reach the target, phishing and especially spear phishing -- phishing attacks carefully tailored to the individual being targeted -- remain effective.

While most early phishing research did not depend on web crawlers, some of the more recent work has been utilizing them to detect yet-unreported phishing sites. \citet{tian2018needle} suggest using a web crawler, optical character recognition, and machine learning to find phishing websites, looking for websites that are visual duplicates of legitimate websites. Similarly, \citet{nathezhtha2019wc} suggest running continuous web crawls to detect ``zero-day'' phishing attacks -- new phishing sites that haven't yet been reported by users as suspicious. A longitudinal crawling approach allows for the detection of new phishing sites as they appear.

\textbf{Web security.}
For researchers desiring to measure security concerns on the web, web crawls are a strong option. As early as 2004, researchers have been using crawls to scan the web for vulnerable websites. \citet{huang2004non} developed a tool to scan websites for cross-site scripting (XSS) and SQL injection attacks. They identified at least 55 of 1120 scanned sites to be vulnerable to XSS attacks. \citet{viennot2014measurement} use a web crawl of the Google Play store, requiring extensive work to bypass anti-crawling measures. Their results show that OAuth is an ineffective and that malicious users are able to gain unauthorized access to private data and resources. Using a crawl of the top one million websites, \citet{kumar2017security} explored security issues on the web. They showed that 90\% of these sites have external dependencies, 33\% have dependencies which themselves have dependencies and 20\% of sites are loaded from five networks, showing important points of failure. The continued development of these concentrated points of failure are evident -- during the writing of this dissertation, Amazon Web Services, a major cloud provider, suffered an outage which caused substantial economic losses~\cite{cnbc2021awsoutage}.

\textbf{HTTPS deployment.}
A major focus in web security is advancing the deployment of encrypted web traffic. Such encryption is done through the HTTPS protocol. Web crawls are not the only tool for approaching measurements in this space: \citet{felt2017measuring} gained access to browser telemetry data and \citet{chan2018monitoring} gained access to Internet Service Provider network logs, allowing to accurately measure the adoption of HTTPS without the use of a web crawl. The measurements strongly disagree with each other -- suggesting around 35\% of page loads versus well over 60\% of page loads in 2015. In comparison \citet{englehardt2015cookies} use a large-scale web crawl to show that around 17\% of the top 55k websites have adopted HTTPS, and around 9\% of the top 1 million. No single one of these measurements is the ``correct'' picture of HTTPs deployment, but understanding HTTPS deployment at different points in time and from different vantage points helps us understand the areas of adoption that need attention and the progress that has been made.

\textbf{Software security.}
Some studies have used internet measurements to study the security of software. Specifically, since much modern software is accessible from the internet, crawlers can find locate downloadable software for researchers to analyze. An early paper in this area looked at the prevalence of spyware in online software. Using a web crawler to scan 18 million URLs, \citet{moshchuk2006crawler} found 13.4\% of executables contained spyware and 5.9\% of web pages. In a similar vein to \cite{kharche2021study}, \cite{fischer2017stack} studied more broadly whether Stack Overflow code snippets propagated into Android apps, showing that over 1.1k vulnerable code snippets appeared in over 1.3m Android applications. 


\subsection{Privacy}
Consumer privacy on the internet is a central issue in consumer protection. While consumer privacy was not included in Kennedy's original Consumer Bill of Rights, it is included in the United Nations declaration. The academic world has seen some substantial shifts around the theories privacy. One such shift is \citet{nissenbaum2004privacy}'s introduction of the idea of \textit{contextual integrity}, which proposes that users' privacy is determined in part by the flow of information, not entirely on the information itself~\cite{nissenbaum2009privacy}. Another major shift is described by \citet{zuboff2014digital}'s introduction of the idea of a \textit{surveillance capitalism}, which observes that major source of privacy violations in the modern era comes from advances in collection and processing of personal data for profit in the private sector, particularly among advertising companies.

As privacy is an issue centered around information flows and the internet is a medium for the flow information, the internet constitutes a major source of privacy challenges. As such, web crawls can be a powerful tool for studying privacy on the internet, allowing researchers to gain insights into the privacy practices of websites, especially among websites which do not disclose their privacy practices. Some websites do disclose their privacy practices in a document called a \textit{privacy policy}, and crawling these documents can be an important approach to study privacy practices at scale. 

\textbf{Trackers.}
Web crawls have been extensively used in studying privacy practices. Tracking is a common threat to consumer privacy, and is well-suited to study via a web crawl. \citet{roesner2012detecting} conducted a study of 1000 websites to identify the volume of five types of trackers, showing the prevalence of variaous trackers on more popular and less popular websites. \citet{schelter2018ubiquity} examine a 3.5 billion page web crawl based on the 2012 common crawl to show that a small number trackers dominate the market, that similar websites use similar trackers, and that the density and choice of trackers varies by country. \citet{acar2014web} show in a crawl of the top 100,000 websites the extent of various persistent tracking mechanisms, such as Evercookies, fingerprinting, and cookie syncing. \citet{englehardt2016online} use a sophisticated web crawling tool designed for privacy measurements, OpenWPM, to study the prevalence of both stateful and stateless tracking technologies in the top 1 million websites. Comparing across these papers, we see that the frequency of trackers is Google, then Facebook, then Twitter. However, Roesner et al. found trackers from Quantserve and Comscore in their top lists, but such trackers do not recur in later studies. By the Englehardt's measurements, just 3 of the top 20 domains did not belong to Google, Facebook, and Twitter. 

\textbf{Data breaches and leaks.}
When a website's data is compromised by an adversary, this creates a substantial threat to the security and privacy of consumers, although courts are inconsistent in their acknowledgement of this harm~\cite{solove2017risk}. Furthermore, courts have risked introducing new hazards to consumers through lackluster communication and identity management when disbursing the award from a class action lawsuit~\cite{amos2019enhancing} \raNote{I wrote this -- we could add this to the dissertation instead of citing it, but it does not have a crawling component}. Such data breaches may leaked or sold on the internet, further amplifying the potential harm, and websites have cropped up to help consumers identifying potential risks to protect themselves~\cite{hunt2019have}. Several researchers have studied data breaches and leaks with web crawlers. \citet{trabelsi2019monitoring} propose the use of a web crawler to scan for new data breaches, encouraging the use of a longitudinal web crawls for continuous monitoring. Similarly, \citet{liu2020identifying} and \cite{nazah2021unsupervised} use web crawlers to collect data to scan for PII, including crawling the dark web\footnote{The dark web consists of websites that require additional software to access, like Tor. The dark web is sometimes conflated with, but is a subset of, the deep web, the set of all websites not indexed in search engines}.

\textbf{Privacy policies.} Privacy policies are one of the few tools consumer have available to them for understanding the privacy practices of the companies they interact with. Many early studies of privacy policies were performed with hand-collected data. For example, \citet{milne2006longitudinal} compared privacy policies in 2001 and 2003 from a set of 312 websites, finding that privacy policies were becoming more difficult to read. This approach does not scale well, however, so the need for automated collecters has arisen. Concurrent to our work, \citet{srinath2020} performed a large-scale crawl of privacy policies using the CommonCrawl dataset. We further investigate literature surrounding privacy policies in section \ref{sec:rim:related_work}.

%\textbf{Internet of Things.} ????  \cite{mohajeri2019watching}

\subsection{Spam.} 
For this section, we look at spam as unsolicited and unhelpful or untruthful information. Spam is a consumer protection issue as impedes the flow of useful information to consumers, by clogging consumers interactions. In effect, spam is a denial of service attack against consumers' information processing. Spam is delivered across a wide variety of platforms such as email, text messages (also known as Short Message Service or SMS), social media, or phone calls~\cite{malwarebytesspam}. Reducing spam is an active goal of the Federal Communications Commission~\cite{fcc2021acting} and the Federal Trade Commission~\cite{ftc2021canspam,ftc21notice}.

%\textbf{Email spam.} Email spam is one of the earliest studied types of spam on the internet. \raNote{More}

\textbf{Opinion spam.}
Opinion spam is a type of spam that disguises itself as an opinion. Often, the idea is to sway users or recommendation algorithms towards an action desired by the spammer through manufactured opinions. Opinion spam is largely synonymous with fake reviews, although it also includes opinions on platforms other than review platforms; for example on internet forums. One of the earliest works on opinion spam was that of \citet{jindal2008opinion}. They collected nearly 6 million reviews from Aamazon and sought to characterize the reviews and identify opinion spam. They hand-labeled obvious cases of opinion spam (for example, irrelevant reviews), and they used duplication as a proxy for opinion spam to include less obvious cases. Large-scale collection of reviews continued with the works of \citet{mukherjee2013yelp} and \citet{rayana2015collective} who collected reviews from Yelp to understand Yelp's classifier and build their own classifiers. Not all fake reviews studies relied entirely on reviews collected from review platforms -- \citet{ott2011finding} compared online reviews collected from TripAdvisor to fictitious reviews they paid participants to generate. We investigate opinion spam further in section \ref{sec:rim:related_work}.

\textbf{Social media spam.}
One type of spam commonly seen on social media is undisclosed social media endorsements. We categorize this issue as spam because of the untruthful nature of endorsing a product without disclosing the endorsement is paid. This issue is an issue the Federal Trade Commission is actively pursuing~\cite{ftc2021disclosures,ftc21notice}. As an online phenomena, such endorsements naturally lend themselves to study by web crawls. \citet{mathur2018endorsements} use a web crawl to collect a random sample of 515,999 YouTube videos and 2,140,462 Pintrest pins and searched for affiliate links in them. When a video contained an affiliate link, natural language processing techniques were used to determine whether the video or pin disclosed the endorsement. They found about 10\% of videos and 7\% of pins contained disclosures, although this quantity varied significantly by category.

%\textbf{Disinformation.} While disinformation has been a concern on the internet for decades~\cite{forbes2002web}, disinformation has been gaining more traction in recent years \raNote{cite}. ....

%\textbf{Robocalls}


\subsection{Dark patterns.}

Dark patterns are a relatively recent introduction to the consumer protection space. Dark patterns are ``tricks used in websites and apps that make you do things that you didn't mean to.'' Harry Brignull coined the term in 2010~\cite{darkpatternsorg}. One of the earliest academic studies was that of \citet{greenberg2014dark}, which describes the human behaviors (ab)used to make dark patterns effective. \citet{gray2018dark} expanded on this work, systematizing dark patterns by manually searching for data tagged with certain unfriendly design-related keywords. They identified five types of dark patterns: nagging, obstruction, sneaking, interface interference, and forced actions. To bring the study of dark patterns to full scale, \citet{mathur2019dark} used a web crawl to collect taxonimize the dark patterns on 11k shopping websites. They add to the taxonomy of dark patterns new classes: urgency, misdirection, social proof, and scarcity.


\input{chapters/background/sections/privacypolicies_related}
\input{chapters/background/sections/reviews_related}

% Internet safety
% - Phishing \cite{hong2012state}, 
% - \cite{khan2017old} compare fraud in both digital and physical spaces
% - Sim swaps \cite{lee2020empirical}
% - data breaches; awareness of breaches \cite{bhagavatula2021breach}; concerns with data breach settlements \cite{amos2019enhancing}
% - Risks of 2FA \cite{lee2021security}
% Privacy issues
% - Smart home privacy \cite{apthorpe2019evaluating},  IOT inspector\cite{huang2020iot}, user perceptions \cite{zheng2018user}
% - Free vs paid apps \cite{bamberger2020can}
% - Re-identification of psuedonymous data in genomes \cite{malin2004not} in 
% - Privacy policies section 
% - Tracking \cite{acar2014web}
% Opinion spam
% - Deception on the internet was a subject of interest as early as 2002 \cite{forbes2002web} and is an ongoing concern \cite{zeng2020bad} \cite{hounsel2020identifying}
% - Falure to disclose endorsements \cite{mathur2018endorsements}
% - Robo-calls \cite{azad2020socioscope}
% - Reviews section
% Dark patterns
% - First? paper to study \cite{gray2018dark}
% - Large scale study \cite{mathur2019dark}
% ???
%\cite{yeung2021Bad} documents a number of abuses of workers on microtask platforms


% Other approaches to studying consumer protection issues:
% - Monitoring user behavior directly \raNote{Cite Mozilla Rally}
% - User studies