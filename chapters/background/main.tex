\chapter{Background and related work} \label{ch:background}

In this chapter, we cover the background material needed to place this work into context. First, we cover some of the definitions of consumer protection in Section \ref{sec:background:definition}. Then we cover some of the most relevant consumer protection laws and agencies in Section \ref{sec:background:laws}. Next, we cover the important literature for contextualizing our contributions to understanding privacy policies in Section \ref{sec:ppot:related}, and online reviews in Section \ref{sec:rim:related_work}. Finally, we survey the academic literature on other consumer protection issues on the internet, with a focus on exploration of such issues through the lens of web crawls in Section \ref{sec:background:crawls}.

\section{What is consumer protection?} \label{sec:background:definition}

One key definition of consumer protection dates to U.S. President John F. Kennedy's ``Consumer bill of rights'' (CBR) speech. This speech established 4 core rights of consumers \cite{kennedy1962special}:
\begin{enumerate}
\item ``The right to safety''
\item ``The right to be informed''
\item ``The right to choose''
\item ``The right to be heard''
\end{enumerate}

The United Nations Guidelines for Consumer Protection (UNGCP) expanded these guidelines, first in 1985, again in 1999, and revised them in 2015. Currently these guidelines are \cite{un2003conpro}:
\begin{enumerate}
\item ``Access by consumers to essential goods and services''
\item ``The protection of vulnerable and disadvantaged consumers''
\item ``The protection of consumers from hazards to their health and safety''
\item ``The promotion and protection of economics interests of consumers''
\item ``Access by consumers to adequate information to enable them to make informed choices  according to individual wishes and needs''
\item ``Consumer education, including education on the environmental, social, and economic consequences of consumer choice''
\item ``Availability of effective consumer dispute resolution and redress''
\item ``Freedom to form consumer and other relevant group or organizations to present their views in decision-making processes affecting them''
\item ``The promotion of sustainable consumption patterns''
\item ``A level of protection for consumer using electronic commerce that is not less than that afforded in other forms of commerce''
\item ``The protection of consumer privacy and the global free flow of information''
\end{enumerate}

In Chapters \ref{ch:ppot} and \ref{ch:rim}, we will be focused on two areas of interest on the internet: privacy policies and online reviews. From a consumer protection standpoint, privacy policies serve a number of roles. They inform consumers about company practices, they help protect consumer privacy through transparency, and, in some cases, provide options for managing disputes (see above: CBR \#2,3,4; UNGCP \#5, 7, 11). Online reviews also help inform consumers, ensure free flow of information, and help consumers promote their economic interests (see above: CBR \#2,4; UNGCP \#4, 5, 7, 11). 


\section{Consumer protection laws} \label{sec:background:laws}
In this section we will highlight the most relevant consumer protection laws and agencies to our work.

In the United States, the federal agency which enforces many of the laws most relevant to the issues which we focus on in this dissertation is the Federal Trade Commission (FTC), established by the Federal Trade Commission Act~\cite{ftcact}.

One of the earliest regulatory actions in online privacy was the FTC's letter that threatened legal action against KidsCom, a website for 5-14 year old children~\cite{ftc1997principles}. The FTC stated that failure to disclose the marketing uses of collected information from the children constituted a violation of Section 5 of the FTC Act, which regulates unfair and deceptive acts and practices. This letter was soon followed by legislation from Congress: the Children's Online Privacy Protection Act ~\cite{coppa}. Unfortunately, compliance with COPPA is still an uphill battle; \citet{reyes2018won} found that a majority of child-directed apps contain potential COPPA violations, although they used a broad definition of what constitutes a violation. More recently in the US, states have expanded their role in online privacy regulation. The California Online Privacy Protection Act (CalOPPA) required online companies to include a privacy policy~\cite{caloppa}, and the California Consumer Privacy Act (CCPA) introduced a protections for consumers' personal information~\cite{ccpaANDcpra}. While CCPA and CalOPPA are part of California law and not US federal law, they may raise the standards of privacy for those in other states, perhaps by inspiring other states to follow suit or by creating burden on companies that wish to differentiate privacy practices by location.

Perhaps the most significant privacy law worldwide is the European Union's General Data Protection Regulation (GDPR). This legislation provided a massive overhaul to the privacy rights of European citizens~\cite{gdpr,hoofnagle2019european}. The GDPR includes many privacy protections, including giving consumers the right to object to the use of their data for direct marketing and under some other circumstances, the right to access their data, and the right to erase their data.

We cover the laws around consumer privacy in more detail in Section \ref{sec:ppot:background}


\input{chapters/background/sections/privacypolicies_related}
\input{chapters/background/sections/reviews_related}

\section{Web crawls to study consumer protection issues} \label{sec:background:crawls}
In this section we will more broadly survey literature in which web crawls have been useful in studying issues of consumer protection, especially on the internet. \citet{ahmad2020apophanies} show that nearly 16\% of 2015-2018 publications at major internet measurement, security, privacy, and networking conferences depend on data obtained from a web crawl. We will examine how internet measurements, especially web crawls, are important in shaping our understanding of consumer protection issues; specifically security, privacy, spam, and dark patterns. In almost every web-crawler based study we examined, the data was collected in a single pass, with no comparison points of the same page being taken again at a later time.

Web crawlers as a research tool face limitations worth considering. The configuration of crawler and the underlying browser, especially configurations like ``headless mode'' (with no user interface) or blocking JavaScript execution; the vantage point, or location, of the crawler; and other variables can shape the crawler's observations~\cite{jueckstock2021towards}. This may mean the crawler's experience is not consistent with a real user experience. Nevertheless, web crawlers are a powerful tool capable of measuring the web at scale, and remains important part of many web measurement methodologies.

\subsection{Security}
Internet measurements are an important tool for studying security problems at scale. When researchers identify a vulnerability, they may wish to understand how widespread the vulnerability is. This can help researchers understand the degree of exposure the public faces to the vulnerability.

\textbf{Cryptography.} One such area of focus is cryptographic vulnerabilities. \citet{heninger2012mining} showed that many websites' RSA keys are vulnerable to attack, for example sharing a common factor with another key. This can be used to recover the private keys for these websites, allowing anyone with that key to impersonate the website. The team scanned the entire IPv4 address space to collect 11 million public keys from TLS and SSH servers and found at least 0.5\% of TLS hosts and 1.6\% of SSH hosts vulnerable to attack. \citet{kharche2021study} used pre-crawled data from Internet Archive to explore the security of cryptography algorithms presented on Stack Overflow, a platform where programmers exchange knowledge such as code snippets. They found several instances of posts on Stack Overflow containing references to outdated cryptography algorithms with no community warnings that the posts were outdated. These errors may propagate into projects containing those snippets. \citet{durumeric2014matter} make use of various longitudinal data collection techniques, including repeated crawls over time, to characterize the impact of the Heartbleed vulnerability and the speed of patching. They collect this data using their Internet scanning tool designed for finding security issues~\cite{durumeric2015search}.
 
\textbf{Phishing.}
Many studies concern internet safety. Phishing is a core issue in internet safety. In a phishing attack, the attacker uses a false identity to fool the user into providing the attacker information. Often this type of attack involves clicking links to websites which are visibly similar to a website the target uses, but are actually controlled by the attacker. \citet{hong2012state} provides an overview of phishing attacks, including the anatomy of attacks, why the attacks are successful, and the scale of the attacks. While some may attribute phishing attacks' success to under-educated users, attackers often take advantage of psychological tactics and poor user interface design~\cite{dhamija2006phishing}. In spite of decades of research into phishing, phishing is far from a solved problem. \citet{das2019sok} outline the modern issues users face from phishing today. While a vast array of machine learning tools have been developed to detect phishing attempts before they reach the target, phishing and especially spear phishing---phishing attacks carefully tailored to the individual being targeted---remain effective. \citet{lain2021phishing} measured the effectiveness of various mitigations during a 15 month study, showing that warnings are effective, while some types of training can be detrimental.

While most early phishing research did not depend on web crawlers, some of the more recent work has been utilizing them to detect yet-unreported phishing sites. \citet{tian2018needle} suggest using a web crawler, optical character recognition, and machine learning to find phishing websites, looking for websites that are visual duplicates of legitimate websites. Similarly, \citet{nathezhtha2019wc} suggest running continuous web crawls to detect ``zero-day'' phishing attacks---new phishing sites that have not yet been reported by users as suspicious. A longitudinal crawling approach allows for the detection of new phishing sites as they appear. Similarly, \citet{roberts2019you} study domain impersonation over a four year study period, tracking the usage over time of multiple techniques for creating phishing URLs. 

\textbf{Web security.}
For researchers desiring to measure security concerns on the web, web crawls are a strong option. As early as 2004, researchers have been using crawls to scan the web for vulnerable websites. \citet{huang2004non} developed a tool to scan websites for cross-site scripting (XSS) and SQL injection attacks. They identified at least 55 of 1120 scanned sites to be vulnerable to XSS attacks. \citet{viennot2014measurement} use a web crawl of the Google Play store, requiring extensive work to bypass anti-crawling measures. Their results show that OAuth is ineffective and that malicious users are able to gain unauthorized access to private data and resources. Using a crawl of the top one million websites, \citet{kumar2017security} explored security issues on the web. They showed that 90\% of these sites have external dependencies, 33\% have dependencies which themselves have dependencies and 20\% of sites are loaded from five networks, showing important points of failure. The continued development of these concentrated points of failure are evident---during the writing of this dissertation, Amazon Web Services, a major cloud provider, suffered an outage which caused substantial economic losses~\cite{cnbc2021awsoutage}.

\textbf{HTTPS deployment.}
A major focus in web security is advancing the deployment of encrypted web traffic. Such encryption is done through the HTTPS protocol. Web crawls are not the only tool for approaching measurements in this space: \citet{felt2017measuring} gained access to browser telemetry data and \citet{chan2018monitoring} gained access to Internet Service Provider network logs, allowing accurate measure of the adoption of HTTPS without the use of a web crawl. The measurements strongly disagree with each other---suggesting around 35\% of page loads versus well over 60\% of page loads in 2015. In comparison \citet{englehardt2015cookies} use a large-scale web crawl to show that around 17\% of the top 55k websites have adopted HTTPS, and around 9\% of the top 1 million. No single one of these measurements is the ``correct'' picture of HTTPS deployment, but understanding HTTPS deployment at different points in time and from different vantage points helps us understand the areas of adoption that need attention and the progress that has been made.

\textbf{Software security.}
Some studies have used internet measurements to study the security of software. Specifically, since much modern software is accessible from the internet, crawlers can locate downloadable software for researchers to analyze. An early paper in this area looked at the prevalence of spyware in online software. Using a web crawler to scan 18 million URLs, \citet{moshchuk2006crawler} found 13.4\% of executables and 5.9\% of web pages contained spyware. In a similar vein to \citet{kharche2021study}, \citet{fischer2017stack} studied more broadly whether Stack Overflow code snippets propagated into Android apps, showing that over 1.1k vulnerable code snippets appeared in over 1.3m Android applications. 


\subsection{Privacy}
Consumer privacy on the internet is a central issue in consumer protection. While consumer privacy was not included in Kennedy's original Consumer Bill of Rights, it is included in the United Nations declaration. The academic world has seen some substantial shifts around the theories of privacy. One such shift is \cites{nissenbaum2004privacy} introduction of the idea of \textit{contextual integrity}, which proposes that users' privacy is determined in part by the flow of information, not entirely on the information itself~\cite{nissenbaum2009privacy}. Another recent shift is described by \cites{zuboff2014digital} introduction of the idea of a \textit{surveillance capitalism}, which observes that a major source of privacy violations in the modern era comes from advances in collection and processing of personal data for profit in the private sector, particularly among advertising companies.

As privacy is an issue centered around information flows and the internet is a medium for the flow of information, the internet constitutes a major source of privacy challenges. As such, web crawls can be a powerful tool for studying privacy on the internet, allowing researchers to gain insights into the privacy practices of websites, especially among websites which do not disclose their privacy practices. Some websites do disclose their privacy practices in a document called a \textit{privacy policy}, and crawling these documents can be an important approach to study privacy practices at scale. 

\textbf{Trackers.}
Web crawls have been extensively used in studying privacy practices. Tracking is a common threat to consumer privacy, and is well-suited to study via a web crawl. \citet{roesner2012detecting} conducted a study of 1000 websites to identify the volume of five types of trackers, showing the prevalence of variaous trackers on more popular and less popular websites. \citet{schelter2018ubiquity} examine a 3.5 billion page web crawl based on the 2012 common crawl to show that a small number trackers dominate the market, that similar websites use similar trackers, and that the density and choice of trackers varies by country. \citet{acar2014web} show in a crawl of the top 100,000 websites the extent of various persistent tracking mechanisms, such as Evercookies, fingerprinting, and cookie syncing. \citet{englehardt2016online} use a sophisticated web crawling tool designed for privacy measurements, OpenWPM, to study the prevalence of both stateful and stateless tracking technologies in the top 1 million websites. Comparing across these papers, we see that the frequency of trackers is Google, then Facebook, then Twitter. However, \citet{roesner2012detecting} found trackers from Quantserve and Comscore in their top lists, but such trackers do not recur in later studies. By the Englehardt's measurements, just 3 of the top 20 domains were not owned by Google, Facebook, and Twitter. Some work has made use of longitudinal collection to study online tracking. \citet{sorensen2019before} use a longitudinal web crawl to study how third party presence changed after the implementation of GDPR, and \citet{krishnamurthy2009privacy} use longitudinal web crawls to show the growth of major web tracking service providers.

\textbf{Data breaches and leaks.}
When a website's data is compromised by an adversary, this creates a substantial threat to the security and privacy of consumers, although courts are inconsistent in their acknowledgement of this harm~\cite{solove2017risk}. Furthermore, the legal system has risked introducing new hazards to consumers through lackluster communication and identity management when disbursing the award from a class action lawsuit~\cite{amos2019enhancing}. Breached data may be leaked or sold on the internet, further amplifying the potential harm, and websites have cropped up to help consumers identify potential risks to protect themselves~\cite{hunt2019have}. Several researchers have studied data breaches and leaks with web crawlers. \citet{trabelsi2019monitoring} propose the use of a web crawler to scan for new data breaches, encouraging the use of longitudinal web crawls for continuous monitoring. Similarly, \citet{liu2020identifying} and \cite{nazah2021unsupervised} use web crawlers to collect data to scan for personally identifiable information, including crawling the dark web\footnote{The dark web consists of websites that require additional software to access, like Tor. The dark web is sometimes conflated with, but is a subset of, the deep web, the set of all websites not indexed in search engines}. Recent Systematization of Knowledge work has identified defenses that help in preventing data breach~\cite{saleem2020sok,saleem2020sok}~and mechanisms for evaluating the harm from breaches~\cite{saleem2020sok}.

\textbf{Privacy policies.} Privacy policies are one of the few tools consumers have available to them for understanding the privacy practices of the companies they interact with. Many early studies of privacy policies were performed with hand-collected data. For example, \citet{milne2006longitudinal} compared privacy policies in 2001 and 2003 from a set of 312 websites, finding that privacy policies were becoming more difficult to read. This approach does not scale well, however, so the need for automated collection has arisen. Concurrent to our work, \citet{srinath2020} performed a large-scale crawl of privacy policies using the CommonCrawl dataset. We further investigate literature surrounding privacy policies in Section \ref{sec:rim:related_work}.

%\textbf{Internet of Things.} ????  \cite{mohajeri2019watching}

\subsection{Spam} 
For this section, we look at spam as unsolicited and unhelpful or untruthful information, which is likely broader than most of the literature would use. Spam is a consumer protection issue as it impedes the flow of useful information to consumers, by clogging consumers' interactions. In effect, spam is a denial of service attack against consumers' information processing. Spam is delivered across a wide variety of platforms such as email, text messages (also known as Short Message Service or SMS), social media, or phone calls~\cite{malwarebytesspam}. Reducing spam is an active goal of the Federal Communications Commission~\cite{fcc2021acting} and the Federal Trade Commission~\cite{ftc2021canspam,ftc21notice}.


\textbf{Opinion spam.}
Opinion spam is a type of spam that disguises itself as an opinion. Often, the idea is to sway users or recommendation algorithms towards an action desired by the spammer through manufactured opinions. Opinion spam is largely synonymous with fake reviews, although it also includes opinions on platforms other than review platforms; for example on internet forums. One of the earliest works on opinion spam was that of \citet{jindal2008opinion}. They collected nearly 6 million reviews from Amazon and sought to characterize the reviews and identify opinion spam. They hand-labeled obvious cases of opinion spam (for example, irrelevant reviews), and they used duplication as a proxy for opinion spam to include less obvious cases. Large-scale collection of reviews continued with the works of \citet{mukherjee2013yelp} and \citet{rayana2015collective} who collected reviews from Yelp to understand Yelp's classifier and build their own classifiers. Not all fake reviews studies relied entirely on reviews collected from review platforms---\citet{ott2011finding} compared online reviews collected from TripAdvisor to fictitious reviews they paid participants to generate. We investigate opinion spam further in Section \ref{sec:rim:related_work}.

\textbf{Social media spam.}
One type of spam commonly seen on social media is undisclosed social media endorsements. We categorize this issue as spam because of the untruthful nature of endorsing a product without disclosing the endorsement is paid. This is an issue the Federal Trade Commission is actively pursuing~\cite{ftc2021disclosures,ftc21notice}. As an online phenomena, such endorsements naturally lend themselves to study by web crawls. \citet{mathur2018endorsements} use a web crawl to collect a random sample of 515,999 YouTube videos and 2,140,462 Pintrest pins and searched for affiliate links in them. When a video contained an affiliate link, natural language processing techniques were used to determine whether the video or pin disclosed the endorsement. They found about 10\% of videos and 7\% of pins contained disclosures, although this quantity varied significantly by category.


\subsection{Dark patterns}

Dark patterns are a relatively recent introduction to the consumer protection space. There are many definitions for dark patterns~\cite{mathur2019dark}, however the definition from Harry Brignull, who coined the term in 2010, is ``tricks used in websites and apps that make you do things that you didn't mean to.''~\cite{darkpatternsorg} One of the earliest academic studies was that of \citet{greenberg2014dark}, which describes the human behaviors (ab)used to make dark patterns effective. \citet{gray2018dark} expanded on this work, systematizing dark patterns by manually searching for data tagged with certain unfriendly design-related keywords. They identified five types of dark patterns: nagging, obstruction, sneaking, interface interference, and forced actions. To bring the study of dark patterns to full scale, \citet{mathur2019dark} used a web crawl to collect and taxonimize the dark patterns on 11k shopping websites. They add to the taxonomy of dark patterns new classes: urgency, misdirection, social proof, and scarcity.


